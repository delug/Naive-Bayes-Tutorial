{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQF8W1DPDq52"
   },
   "source": [
    "# What is a Naive Bayes Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Bayes Theorem?\n",
    "\n",
    "Bayes Theorem is a way to find conditional probabilities when we know the values for related probabilities. A conditional probability is the probability of event A occuring when we know that event B has already occured. We won't go over the derivation here, but you can read more about it here (it's pretty simple!): http://www.hep.upenn.edu/~johnda/Papers/Bayes.pdf  \n",
    "\n",
    "We write the Bayes Theorem as follows:  \n",
    "  \n",
    "$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$  \n",
    "  \n",
    "We read $P(A|B)$ as the probability of A given B, or the conditional probability of A knowing that B has already occured.  \n",
    "  \n",
    "Let's look at an example. Say you are at a picnic with your friends, and the weather suddenly becomes cloudy. You would like to know if this will lead to rain. In other words, you would like to find the conditional probability that it will rain given it is clowdy ($P(Rain|Cloudy)$). Let's say that 40% of the days are cloudy and 20% of days are rainy ($P(Cloudy) = 0.4$ and $P(Rain) = 0.2$). We also know that on days that rainy days are always cloudy ($P(Cloudy|Rain)=1.0$). Now, we have all the information we need to use Bayes Theorem to find $P(Rain|Cloudy)$.  \n",
    "  \n",
    "$P(Rain|Cloudy)=\\frac{P(Rain)P(Cloudy|Rain)}{P(Cloudy)}=\\frac{0.2 * 1.0}{0.4}=0.5 $ \n",
    "  \n",
    "Now we know there is a fifty percent chance that it will rain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEAjjuXCFVeR"
   },
   "source": [
    "## Premise of Naive Bayes\n",
    "\n",
    "Let's say we want to make a model which will try to classify all days as rainy or dry. Instead of just using one variable (if it is cloudy or not), we wish to incorporate many other variables as well (did it rain yesterday, temperature, humidity, etc.). We again can approach this classification task probabalistically like we did with the picnic example. To do so, we introdue the Naive Bayes Classifier.\n",
    "\n",
    "The Naive Bayes classifier is a simple yet powerful probabalistic machine learning model. Using Bayes theorem and conditional probabality calculations, we can find estimates for probabilities that a specific object is of a certain class. Later, we will show how the Naive Bayes classifier can be used for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg4-FUfKxvTB"
   },
   "source": [
    "## Derivation\n",
    "\n",
    "Lets say we have an object $x$ represented as an n-dimensional vector $x = <x_{1}, x_{2}, ..., x_{n}>$. We would like to classify each object $x$ a probability for being in class $C_{k}$, i.e. we would like to find $P(C_{k}|x_{1}, ..., x_{n})$.This probability is likely intractable for high dimensional data (like long sequences of text). So to find this probability, we must first use Bayes theorem. We can rewrite $P(C_{k}|x_{1}, ..., x_{n})$ as $\\frac{P(C_{k})P(x_{1}, ..., x_{n}|C_{k})}{P(x_{1}, ..., x_{n})}$. In other words, we know $P(C_{k}|x_{1}, ..., x_{n}) \\propto P(C_{k})P(x_{1}, ..., x_{n}|C_{k})=P(x_{1}, ..., x_{n},C_{k})$. We can rewrite this joint proability using the chain rule: \n",
    "$P(x_{1}, ..., x_{n},C_{k})=P(x_{1}|x_{2} ..., x_{n},C_{k})P(x_{2} ..., x_{n},C_{k})=P(x_{1}|x_{2} ..., x_{n},C_{k})P(x_{2}| x_{3}, ..., x_{n},C_{k})P(x_{3}, ..., x_{n},C_{k})=...$ Eventually, we get $P(x_{1}, ..., x_{n},C_{k})=P(x_{1}|x_{2} ..., x_{n},C_{k})...P(x_{n-1}|x_{n},C_{k})p(x_{n}|C_{k})p(C_{k})$\n",
    "\n",
    "This is where we make the naive independent assumption. The naive independent assumption is that each feature in $x$ is mutually independent. This dramatically simplifies the probability expression we are calculating, as now we can assume that $P(x_{i}|x_{i+1},...,x_{n},C_{k})=P(x_{i}|C_{k})$. Now let's go back to our derivation. $P(x_{1}, ..., x_{n},C_{k})=P(x_{1}|x_{2} ..., x_{n},C_{k})...P(x_{n-1}|x_{n},C_{k})p(x_{n}|C_{k})p(C_{k}) \\propto P(x_{1}|C_{k})...P(x_{n-1}|C_{k})p(x_{n}|C_{k})p(C_{k})=p(C_{k})\\Pi p(x_i|C_k)$.  \n",
    "Notice that we move from an equals sign to a proportionality sign. Why is that?\n",
    "\n",
    "Let's quickly backtrack. We began by wanting to find the conditional probability that an object $x$ was of class $C_k$. After we applied Bayes theorem, did some derivations, and made the naive independent assumption, we found that this probability is proportional to $p(C_{k})\\Pi p(x_i|C_k)$. In plain English, this expression is the probability that any given document is of class $C_{k}$ multiplied by the product that a feature in class $C_k$ is $x_i$. Let's work on an example to combine theory with practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNqqd2mL0D0V"
   },
   "source": [
    "## Example\n",
    "\n",
    "Suppose we would like to try to classify emails as spam or \"regular\" depending on it's content. We will be taking a \"bag of words\" approach to analyzing the content of the emails. This means that we are only concerned with the frequency of words' usages, not how they are used or where they are placed. \n",
    "\n",
    "Let's look at the following example (very short) spam message:\n",
    "\n",
    "EMERGENCY - PLEASE WIRE MONEY\n",
    "\n",
    "To calculate the probability that this message is of class spam or regular mail, again we must know the proability that any given message is of class spam or regular and the conditional probability of a given word being present given we know that the message is regular or spam. For now, let's say that we receive 40% spam messages and 60% non-spam messages.\n",
    "\n",
    "Now we must calculate the conditional probability that each word in the message will appear given that the message is spam/regular. How do we do this. For example, say the word \"Emergency\" (we'll ignore the caps lock in the email) appears 20 times in 1000 words for regular messages, and appears 8 times in 200 words for spam messages.  \n",
    "<table>\n",
    "<tr>\n",
    "    <th>Word</th>\n",
    "    <th>Spam Count</th>\n",
    "    <th>Regular Count</th> \n",
    "    <th>Spam Frequency</th>\n",
    "    <th>Regular Frequency</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>Emergency</td> \n",
    "    <td>8</td> \n",
    "    <td>20</td>\n",
    "    <td>.04</td> \n",
    "    <td>.02</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>Please</td> \n",
    "    <td>4</td> \n",
    "    <td>80</td> \n",
    "    <td>.02</td> \n",
    "    <td>.08</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "    <td>Wire</td> \n",
    "    <td>16</td> \n",
    "    <td>10</td> \n",
    "    <td>.08</td> \n",
    "    <td>.01</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "    <td>Money</td> \n",
    "    <td>30</td> \n",
    "    <td>20</td> \n",
    "    <td>.15</td> \n",
    "    <td>.02</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "  \n",
    "This means $P(Emergency | Regular) = 20 / 1000 = .02$ and $P(Emergency | Spam) = 8 / 200 = .04$ Using this same process, we can calculate the conditional probabilities for each word in the example spam message respective of each class.  \n",
    "\n",
    "$P(Please | Regular) = .08$  \n",
    "$P(Wire | Regular) = .01$  \n",
    "$P(Money | Regular) = .02$  \n",
    "$P(Please | Spam) = .02$  \n",
    "$P(Wire | Spam) = .08$  \n",
    "$P(Money | Spam) = .15$  \n",
    "\n",
    "Now, let's think back to the formula for approximating that a message $x$ is of a class $c_{k}$:  \n",
    "$p(C_{k})\\Pi p(x_i|C_k)$  \n",
    "Let's apply this proability to probability this message is spam or a regular email.  \n",
    "$P(Spam|Emergency Please Wire Money) \\propto P(Spam) * P(Emergency | Spam) * P(Please | Spam) * P(Wire | Spam) * P(Money | Spam) = 0.40 * 0.04 * 0.02 * 0.08 * 0.15 = 3.84 * 10^{-6}$  \n",
    "$P(Regular|Emergency Please Wire Money) \\propto P(Regular) * P(Emergency | Regular) * P(Please | Regular) * P(Wire | Regular) * P(Money | Regular) = 0.60 * 0.02 * 0.08 * 0.01 * 0.02 = 1.92 * 10^{-7}$\n",
    "\n",
    "As the value we receive for the spam calculation greater than the value from the regular calculation, we classify the message as spam (which is correct!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the theory behind Naive Bayes, let's go about programming one. We will use the sklearn package's Naive Bayes classifier. This means we shouldn't have to think too much about the theory of Naive Bayes as we program (though making a classifier from scratch would be excellent practice). However, we will have to pay more attention to how we want to pre-process the data we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqKVbCdeyTph"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zM9Jm7cEyMYD"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from random import shuffle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQa3XIzryXOf"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are solving in this tutorial is classifying chunks of text from classic novels by author. Our data is stored in the text folder in our directory. Within the text folder, we have four folders: train, test, train_edited, and test_edited. The train folder contains several books written by the seven novelists we will attempt to classify the works by (Louisa May Alcott, Jane Austen, Arthur Conan Doyle, Nathaniel Hawthorne, Franz Kafka, Herman Melville, Oscar Wilde). The edited versions of each directory contains the same works but with the publishing information at the beginning manually taken out. We will see how this step affects performance at the end of the tutorial.\n",
    "\n",
    "Let's begin by creating some helper functions for reading files and navigating directories. First, let's make a function readFile that will return the text of the file in all lowercase given an appropriate filepath. Second, let's make a function getFilePaths that will return all the files ending with .txt in a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cO56UWjN0EYw"
   },
   "outputs": [],
   "source": [
    "# Input: file path\n",
    "# Output: text of file\n",
    "def readFile(path, lower=True):\n",
    "    with open(path, encoding=\"utf8\") as reader:\n",
    "        text = reader.read()\n",
    "        if lower:\n",
    "            return text.lower()\n",
    "        return text\n",
    "        \n",
    "# Input: name of directory\n",
    "# Output: list of paths for each file in that directory\n",
    "def getFilePaths(directory):\n",
    "    file_paths = []\n",
    "    for f in listdir(directory):\n",
    "        file = join(directory, f)\n",
    "        if isfile(file) and file[-4:] == '.txt':\n",
    "            file_paths.append(file)\n",
    "    return file_paths   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's important to seperate each .txt file (containing an entire book) into manageable chunks of 500 words. Let's walk through this function. We return to corresponding arrays: authors (containing the author for each chunk) and text_seqs (containing the 500 word chunks). We take a list of file paths as input, along with two conditions: remove_sw and remove_punc. The argument remove_sw removes stopwords (words like \"the\" or \"of\" which theoretically do not tell you too much about an author's distinctive voice) from each chunk of text. For this we use remove_stopwords function from the gensim package. The remove_punc argument, if set to true, indicates to remove all punctuation from each chunk of text. Now, for each file in file_paths, we extract the author from the file name and the text using read file. We then tokenize the text using the nltk package's word_function function. Tokenizing means to seperate a text into each component word (called tokens). Lastly, each tokenizes text is broken into 500-word sequences; the author name and text sequence are then added in order to the authors in tex_seqs lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AF-V-Y_oC2bW"
   },
   "outputs": [],
   "source": [
    "# Input: list of file paths\n",
    "# Output: Tuple mapping authors to all text from all books\n",
    "#   (ex. 'Dickens' -> 'It was the best of times ...\")\n",
    "def makeDict(file_paths, remove_sw=True, remove_punc=True):\n",
    "    authors = []\n",
    "    text_seqs = []\n",
    "    for f in file_paths:\n",
    "        author = f.split('\\\\')[1].split('-')[0]\n",
    "        text = readFile(f)\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        if remove_punc:\n",
    "            tokenized_text = [token for token in tokenized_text if token.isalnum()]\n",
    "        for i in range (0, len(tokenized_text), 500):\n",
    "            authors.append(author)\n",
    "            s = ' '.join(tokenized_text[i:i+500])\n",
    "            if remove_sw:\n",
    "                s = remove_stopwords(s)\n",
    "            text_seqs.append(s)\n",
    "    return (authors, text_seqs)\n",
    "\n",
    "DIR = 'text/train_edited'\n",
    "fp_list = getFilePaths(DIR)\n",
    "d = makeDict(fp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GHxvsakydGp"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our Naive Bayes classifier.In the function makeClassifier, we take the type of model and the X and Y output as inputs and return the fitted model. You see that with sklearn, creating the model and training the model is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aL76FIVm07J6"
   },
   "outputs": [],
   "source": [
    "# Input: training data X (as csr_matrix), and training targets Y (as list)\n",
    "# Output: classifier object fitted to training data\n",
    "def makeClassifier(X, Y):\n",
    "    model = MultinomialNB()\n",
    "    return model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn0cAlI0yn-P"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run expiriments to find which models have the best performance. In the expiriment method, we preprocess the data using the expiriments we trained before. Then, we use the CountVectorizer object from sklearn to tokenize the training and the testing objects. We create a fitted multinomial NB classifier and Bernoulli NB classifier in each expiriment. We finally run each model on the testing data and print and record the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CR89tae63r3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edited = False, remove_stopwords = False, remove_punctuation = False\n",
      "Multinomial\n",
      "accuracy 0.7551020408163265\n",
      "\n",
      "Bernoulli\n",
      "accuracy 0.6908909905425585\n"
     ]
    }
   ],
   "source": [
    "def expiriment(edited, remove_sw, remove_punc):\n",
    "    # Print hyperparamaters\n",
    "    print(\"Edited = \" + str(edited) +\n",
    "          \", remove_stopwords = \" + str(remove_sw) +\n",
    "          \", remove_punctuation = \" + str(remove_punc))\n",
    "    # Make the corpus\n",
    "    # Each document will be the text of all of an author's works combined\n",
    "    if edited:\n",
    "        train_file_paths = getFilePaths(\"text/train_edited\")\n",
    "        test_file_paths = getFilePaths(\"text/test_edited\")\n",
    "    else:\n",
    "        train_file_paths = getFilePaths(\"text/train\")\n",
    "        test_file_paths = getFilePaths(\"text/test\")\n",
    "    train_authors, train_text_seqs = makeDict(train_file_paths, remove_sw, remove_punc)\n",
    "    test_authors, test_text_seqs = makeDict(test_file_paths, remove_sw, remove_punc)\n",
    "      \n",
    "    corpus = train_text_seqs\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(corpus)\n",
    "    Y_train_targets = train_authors\n",
    "    multiClf = makeClassifier(\"Multinomial\", X_train_counts, Y_train_targets)\n",
    "    test_corpus = test_text_seqs\n",
    "    X_test_counts = count_vect.transform(test_corpus)\n",
    "    Y_test_targets = test_authors\n",
    "    \n",
    "    Y_test_predicted_multi = multiClf.predict(X_test_counts)\n",
    "    \n",
    "    print(\"Multinomial\")\n",
    "    print(\"accuracy\", np.mean(Y_test_targets == Y_test_predicted_multi))\n",
    "    return\n",
    "\n",
    "expiriment(False, False, False)\n",
    "#expiriment(False, False, True)\n",
    "#expiriment(False, True, False)\n",
    "#expiriment(False, True, True)\n",
    "#expiriment(True, False, False)\n",
    "#expiriment(True, False, True)\n",
    "#expiriment(True, True, False)\n",
    "#expiriment(True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may note that there are three different parameters or configurations that affect model performance: pre-edting, removing stopwords, and removing punctuation. It's worth the time to look at the results and think about how the choices behind these paramaters impact performance. For example, why would removing stopwords affect the performance of an NB classifier? Why would pre-editing the text?\n",
    "\n",
    "Hopefully you are left with a greater understanding of how Naive Bayes works, and perhaps, an appreciation for how such a simple model can perform decently at such a sophisticated task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "Wikipedia Article -- https://en.wikipedia.org/wiki/Naive_Bayes_classifier  \n",
    "StatQuest Video -- https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer  \n",
    "Dr. Maier's CSCI 6800 Course at UGA (Data Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NB_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
